{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a3dd012e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\elang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import requests\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b538325b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nama_tempat</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>Geprek Kak Rose</td>\n",
       "      <td>Tukang parkirnya cmn mau duitnya doang. Habis ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7568</th>\n",
       "      <td>Mie Gacoan Stasiun Kota Malang</td>\n",
       "      <td>layanan nya bagusss, makanan nyaa enakk toppp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7261</th>\n",
       "      <td>Mie Gacoan Sawojajar Kota Malang</td>\n",
       "      <td>Taste tetap terjaga. Isi Pangsit agak kecil is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2931</th>\n",
       "      <td>Geprek Kak Rose Suhat Malang</td>\n",
       "      <td>Enak rasanya, luas juga untuk lantai 2 nya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>Geprek Kak Rose</td>\n",
       "      <td>Cocok dengan harga</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           nama_tempat  \\\n",
       "1294                   Geprek Kak Rose   \n",
       "7568    Mie Gacoan Stasiun Kota Malang   \n",
       "7261  Mie Gacoan Sawojajar Kota Malang   \n",
       "2931      Geprek Kak Rose Suhat Malang   \n",
       "1503                   Geprek Kak Rose   \n",
       "\n",
       "                                                 review  \n",
       "1294  Tukang parkirnya cmn mau duitnya doang. Habis ...  \n",
       "7568      layanan nya bagusss, makanan nyaa enakk toppp  \n",
       "7261  Taste tetap terjaga. Isi Pangsit agak kecil is...  \n",
       "2931         Enak rasanya, luas juga untuk lantai 2 nya  \n",
       "1503                                 Cocok dengan harga  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data_clean/all_reviews_merged.csv\")\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f8983b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningText(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text)\n",
    "    text = re.sub(r'#[A-Za-z0-9]+', '', text)\n",
    "    text = re.sub(r\"http\\S+\", '', text)\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.strip(' ')\n",
    "    return text\n",
    "\n",
    "\n",
    "def casefoldingText(text):\n",
    "    return (text or '').lower()\n",
    "\n",
    "\n",
    "def normalize_repeated_chars(text):\n",
    "    # Ubah huruf yang sama berurutan lebih dari 2 kali menjadi tepat 2 kali, contoh: \"enakkkk\" -> \"enakkk\" -> \"enakk\"\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1\\1', (text or ''))\n",
    "\n",
    "\n",
    "def tokenizingText(text):\n",
    "    text = (text or '').strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    # IndoBERT subword tokenization\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "\n",
    "def filteringText(tokens):\n",
    "    listStopwords = set(stopwords.words('indonesian'))\n",
    "    listStopwords.update(set(stopwords.words('english')))\n",
    "    additional_stopwords = ['iya','yaa','gk','gak','g','dr','nya','na','sih','ku','di','ga','ya','gaa','loh','kah','woi','woii','woy','pas','c','deh','eh']\n",
    "    listStopwords.update(additional_stopwords)\n",
    "    return [word for word in tokens if word not in listStopwords]\n",
    "\n",
    "\n",
    "def toSentence(list_words):\n",
    "    return ' '.join(word for word in list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f334e172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4417"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _get_json_with_retries(url, params=None, retries=3, timeout=30, backoff=1.5):\n",
    "    last_err = None\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            resp = requests.get(url, params=params, timeout=timeout)\n",
    "            if resp.status_code == 200:\n",
    "                try:\n",
    "                    return resp.json()\n",
    "                except Exception as je:\n",
    "                    last_err = je\n",
    "            else:\n",
    "                last_err = RuntimeError(f\"HTTP {resp.status_code}: {resp.text[:200]}\")\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "        time.sleep(backoff ** i)\n",
    "    raise RuntimeError(f\"Failed to GET JSON from {url} after {retries} retries: {last_err}\")\n",
    "\n",
    "\n",
    "def load_slangwords_from_hf(cache_path=\"data_clean/slangwords.json\", max_rows=None):\n",
    "    base = \"https://datasets-server.huggingface.co\"\n",
    "    ds = \"theonlydo/indonesia-slang\"\n",
    "    cfg = \"default\"\n",
    "    split = \"train\"\n",
    "\n",
    "    try:\n",
    "        if cache_path and os.path.exists(cache_path):\n",
    "            with open(cache_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                cached = json.load(f)\n",
    "                if isinstance(cached, dict) and cached:\n",
    "                    return {str(k).lower(): str(v).lower() for k, v in cached.items()}\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if cache_path:\n",
    "        os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n",
    "\n",
    "    meta = _get_json_with_retries(\n",
    "        f\"{base}/rows\",\n",
    "        params={\"dataset\": ds, \"config\": cfg, \"split\": split, \"offset\": 0, \"length\": 1},\n",
    "        retries=4,\n",
    "        timeout=30,\n",
    "    )\n",
    "    total = int(meta.get(\"num_rows_total\", 0))\n",
    "    if total <= 0:\n",
    "        raise RuntimeError(\"Dataset returned zero rows or missing num_rows_total\")\n",
    "\n",
    "    if isinstance(max_rows, int) and max_rows > 0:\n",
    "        total_to_fetch = min(total, max_rows)\n",
    "    else:\n",
    "        total_to_fetch = total\n",
    "\n",
    "    slang = {}\n",
    "    offset = 0\n",
    "    length = 100\n",
    "    fetched = 0\n",
    "\n",
    "    while fetched < total_to_fetch:\n",
    "        remaining = total_to_fetch - fetched\n",
    "        request_length = min(length, remaining)\n",
    "        data = _get_json_with_retries(\n",
    "            f\"{base}/rows\",\n",
    "            params={\n",
    "                \"dataset\": ds,\n",
    "                \"config\": cfg,\n",
    "                \"split\": split,\n",
    "                \"offset\": offset,\n",
    "                \"length\": request_length,\n",
    "            },\n",
    "            retries=4,\n",
    "            timeout=60,\n",
    "        )\n",
    "        rows = data.get(\"rows\", [])\n",
    "        for r in rows:\n",
    "            row = r.get(\"row\", {})\n",
    "            s = str(row.get(\"slang\", \"\")).strip().lower()\n",
    "            f = str(row.get(\"formal\", \"\")).strip().lower()\n",
    "            if s and f:\n",
    "                slang[s] = f\n",
    "        batch = len(rows)\n",
    "        fetched += batch\n",
    "        offset += batch\n",
    "        if batch == 0:\n",
    "            break\n",
    "\n",
    "    if not slang:\n",
    "        raise RuntimeError(\"No slang entries loaded from Hugging Face dataset\")\n",
    "\n",
    "    if cache_path:\n",
    "        try:\n",
    "            with open(cache_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(slang, f, ensure_ascii=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return slang\n",
    "\n",
    "\n",
    "slangwords = load_slangwords_from_hf()\n",
    "len(slangwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "535ebf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_slangwords(text):\n",
    "    words = text.split()\n",
    "    fixed_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if word.lower() in slangwords:\n",
    "            fixed_words.append(slangwords[word.lower()])\n",
    "        else:\n",
    "            fixed_words.append(word)\n",
    "\n",
    "    fixed_text = ' '.join(fixed_words)\n",
    "    return fixed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a8e4e9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membersihkan teks dan menyimpannya di kolom 'text_clean'\n",
    "df['text_clean'] = df['review'].apply(cleaningText)\n",
    "\n",
    "# Mengubah huruf dalam teks menjadi huruf kecil dan menyimpannya di 'text_casefoldingText'\n",
    "df['text_casefoldingText'] = df['text_clean'].apply(casefoldingText)\n",
    "\n",
    "# Normalisasi huruf berurutan (>2 jadi 2), sebelum slangwords\n",
    "df['text_normrepeat'] = df['text_casefoldingText'].apply(normalize_repeated_chars)\n",
    "\n",
    "# Mengganti kata-kata slang dengan kata-kata standar dan menyimpannya di 'text_slangwords'\n",
    "df['text_slangwords'] = df['text_normrepeat'].apply(fix_slangwords)\n",
    "\n",
    "# Memecah teks menjadi token (kata-kata) dan menyimpannya di 'text_tokenizingText'\n",
    "df['text_tokenizingText'] = df['text_slangwords'].apply(tokenizingText)\n",
    "\n",
    "# Menghapus kata-kata stop (kata-kata umum) dan menyimpannya di 'text_stopword'\n",
    "df['text_stopword'] = df['text_tokenizingText'].apply(filteringText)\n",
    "\n",
    "# Menggabungkan token-token menjadi kalimat dan menyimpannya di 'text_akhir'\n",
    "df['text_akhir'] = df['text_stopword'].apply(toSentence)\n",
    "\n",
    "# Simpan versi bersih yang dipakai untuk inference selanjutnya\n",
    "df['text_clean'] = df['text_slangwords']\n",
    "\n",
    "# Output akhir\n",
    "df['sentence'] = df['text_akhir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b3cd0048",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)\n",
    "output_cols = ['text_clean']\n",
    "df_out = df[output_cols].copy()\n",
    "output_csv = \"data_clean/all_reviews_cleaned.csv\"\n",
    "df_out.to_csv(output_csv, index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
