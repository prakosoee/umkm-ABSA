{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fab3dd35",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing for Google Maps Reviews\n",
    "\n",
    "This notebook contains the data cleaning and preprocessing pipeline for Google Maps reviews. It includes the following steps:\n",
    "\n",
    "1. **Setup and Dependencies** - Importing required libraries and setting up the environment\n",
    "2. **Loading Data** - Loading the raw review data\n",
    "3. **Text Preprocessing** - Cleaning and normalizing the text data\n",
    "4. **Translation** - Translating English words to Indonesian\n",
    "5. **Tokenization** - Breaking down text into tokens\n",
    "6. **Stopword Removal** - Removing common words\n",
    "7. **Final Processing** - Preparing the final cleaned text\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3dd012e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\elang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup and Dependencies\n",
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize BERT tokenizer for Indonesian language\n",
    "tokenizer = BertTokenizer.from_pretrained(\"indobenchmark/indobert-large-p2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d08a084",
   "metadata": {},
   "source": [
    "# 2. Loading Data\n",
    "\n",
    "In this section, we load the raw review data from the CSV file and take an initial look at the dataset structure.\n",
    "\n",
    "```python\n",
    "# Load the dataset from CSV file\n",
    "file_path = \"data_clean/all_reviews_merged.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9105f202",
   "metadata": {},
   "source": [
    "# 2. Loading Data\n",
    "\n",
    "In this section, we load the raw review data from the CSV file and take an initial look at the dataset structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f31d42a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (9610, 2)\n",
      "\n",
      "Sample of the dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nama_tempat</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5797</th>\n",
       "      <td>Kopi Studio 24 Panglima Sudirman</td>\n",
       "      <td>tempatnya bagus harganya juga ramah dikantong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>Kopi Studio 24</td>\n",
       "      <td>makanan dan minuman ada di harga yg sangat ter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8022</th>\n",
       "      <td>Mie Gacoan Suhat</td>\n",
       "      <td>cakep tempatnya bersih layanannya cepat, makan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6954</th>\n",
       "      <td>Mie Gacoan Sawojajar Kota Malang</td>\n",
       "      <td>Sangat ramai, tapi tidak menunggu lama karena ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3925</th>\n",
       "      <td>Kopi Studio 24</td>\n",
       "      <td>Minumannya enak banget sama murah</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           nama_tempat  \\\n",
       "5797  Kopi Studio 24 Panglima Sudirman   \n",
       "3462                    Kopi Studio 24   \n",
       "8022                  Mie Gacoan Suhat   \n",
       "6954  Mie Gacoan Sawojajar Kota Malang   \n",
       "3925                    Kopi Studio 24   \n",
       "\n",
       "                                                 review  \n",
       "5797      tempatnya bagus harganya juga ramah dikantong  \n",
       "3462  makanan dan minuman ada di harga yg sangat ter...  \n",
       "8022  cakep tempatnya bersih layanannya cepat, makan...  \n",
       "6954  Sangat ramai, tapi tidak menunggu lama karena ...  \n",
       "3925                  Minumannya enak banget sama murah  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset from CSV file\n",
    "file_path = \"data_clean/all_reviews_merged.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nSample of the dataset:\")\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0bf503",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing Functions\n",
    "\n",
    "This section contains all the necessary functions for text cleaning and preprocessing. These functions will be applied to the review text in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ec5d925",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Translation Functions (HF Transformers: en -> id)\n",
    "from wordfreq import zipf_frequency\n",
    "from transformers import pipeline as hf_pipeline\n",
    "\n",
    "# Initialize HF translator once (English -> Indonesian)\n",
    "# Force PyTorch backend to avoid TensorFlow/Keras dependency\n",
    "try:\n",
    "    translator_en_id = hf_pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-id\", framework=\"pt\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"Failed to initialize translation pipeline with PyTorch. \"\n",
    "        \"Please ensure PyTorch and sentencepiece are installed.\\n\"\n",
    "        \"Try: pip install -U torch sentencepiece transformers\\n\"\n",
    "        f\"Original error: {e}\"\n",
    "    )\n",
    "\n",
    "# Higher threshold to reduce false positives for Indonesian words\n",
    "ZIPF_THRESHOLD = 3.0\n",
    "\n",
    "# Simple cache for span translations to avoid repeated calls\n",
    "_translation_cache = {}\n",
    "\n",
    "def is_probably_english_token(token):\n",
    "    \"\"\"\n",
    "    Determine if a token is likely English based on word frequency.\n",
    "    Uses wordfreq.zipf_frequency(token, 'en').\n",
    "    If frequency for 'en' is above threshold, consider it English.\n",
    "    \"\"\"\n",
    "    # Skip numbers, emoticons, or very short non-letter tokens\n",
    "    if not re.search(r'[A-Za-z]', token):\n",
    "        return False\n",
    "    # Remove non-letters from the start/end\n",
    "    cleaned = re.sub(r'^[^A-Za-z]+|[^A-Za-z]+$', '', token)\n",
    "    if len(cleaned) < 2:\n",
    "        return False\n",
    "    # Quick guard: common Indonesian short words\n",
    "    common_id = {\"ya\",\"yg\",\"ygnya\",\"yang\",\"itu\",\"ini\",\"dan\",\"di\",\"ke\",\"kok\",\"lah\",\"sih\",\"mau\",\"tapi\"}\n",
    "    if cleaned.lower() in common_id:\n",
    "        return False\n",
    "    freq = zipf_frequency(cleaned.lower(), 'en')\n",
    "    return freq >= ZIPF_THRESHOLD\n",
    "\n",
    "def split_preserve_delimiters(text):\n",
    "    \"\"\"\n",
    "    Split text into tokens while preserving whitespace and punctuation.\n",
    "    Example: ['Hello', ', ', 'apa', ' ', 'kabar', '?']\n",
    "    \"\"\"\n",
    "    parts = re.findall(r\"[A-Za-z']+|\\d+|\\s+|[^\\w\\s]\", text, flags=re.UNICODE)\n",
    "    return parts\n",
    "\n",
    "def merge_english_spans(parts):\n",
    "    \"\"\"\n",
    "    Merge consecutive English tokens into spans for translation.\n",
    "    Returns a list of dicts with 'type' ('en' or 'other') and 'text'.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    i = 0\n",
    "    while i < len(parts):\n",
    "        p = parts[i]\n",
    "        if is_probably_english_token(p):\n",
    "            span = [p]\n",
    "            i += 1\n",
    "            while i < len(parts) and (is_probably_english_token(parts[i]) or \n",
    "                                       re.match(r\"[-']\", parts[i]) or \n",
    "                                       parts[i].isdigit()):\n",
    "                span.append(parts[i])\n",
    "                i += 1\n",
    "            out.append({'type': 'en', 'text': ''.join(span)})\n",
    "        else:\n",
    "            out.append({'type': 'other', 'text': p})\n",
    "            i += 1\n",
    "    return out\n",
    "\n",
    "def translate_spans(items, max_length=256, batch_size=32):\n",
    "    \"\"\"\n",
    "    Translate only the 'en' spans using HF pipeline (en->id), batched.\n",
    "    Returns the combined text with Indonesian kept as-is.\n",
    "    \"\"\"\n",
    "    # Collect indices and texts for English spans\n",
    "    en_indices = []\n",
    "    en_texts = []\n",
    "    for idx, it in enumerate(items):\n",
    "        if it['type'] == 'en':\n",
    "            en_indices.append(idx)\n",
    "            en_texts.append(it['text'])\n",
    "\n",
    "    # Use cache and figure out which need translation\n",
    "    to_translate = []\n",
    "    map_idx_to_key = {}\n",
    "    for i, text in zip(en_indices, en_texts):\n",
    "        key = text.strip()\n",
    "        map_idx_to_key[i] = key\n",
    "        if key not in _translation_cache:\n",
    "            to_translate.append(key)\n",
    "\n",
    "    # Batch translate missing keys\n",
    "    if to_translate:\n",
    "        results = []\n",
    "        for start in range(0, len(to_translate), batch_size):\n",
    "            batch = to_translate[start:start+batch_size]\n",
    "            outs = translator_en_id(batch, max_length=max_length)\n",
    "            results.extend([o.get('translation_text', '') for o in outs])\n",
    "        for key, translated in zip(to_translate, results):\n",
    "            _translation_cache[key] = translated\n",
    "\n",
    "    # Reconstruct output\n",
    "    out_parts = []\n",
    "    for idx, it in enumerate(items):\n",
    "        if it['type'] == 'en':\n",
    "            key = map_idx_to_key[idx]\n",
    "            out_parts.append(_translation_cache.get(key, it['text']))\n",
    "        else:\n",
    "            out_parts.append(it['text'])\n",
    "    return ''.join(out_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6375dbb4",
   "metadata": {},
   "source": [
    "### 3.2 Slang Word Normalization\n",
    "\n",
    "This function replaces informal/slang words with their standard equivalents using a predefined dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8983b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.1 Text Cleaning Functions\n",
    "\n",
    "def cleaningText(text):\n",
    "    \"\"\"\n",
    "    Clean the input text by removing mentions, hashtags, URLs, numbers, and special characters.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', ' ', text)  # Remove mentions\n",
    "    text = re.sub(r'#[A-Za-z0-9]+', ' ', text)   # Remove hashtags\n",
    "    text = re.sub(r\"http\\S+\", '', text)        # Remove URLs\n",
    "    text = re.sub(r'[0-9]+', '', text)          # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)        # Remove special characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n",
    "    text = text.replace('\\n', ' ')              # Remove newlines\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    text = text.strip(' ')                      # Remove leading/trailing spaces\n",
    "    return text\n",
    "\n",
    "def casefoldingText(text):\n",
    "    \"\"\"Convert text to lowercase.\"\"\"\n",
    "    return (text or '').lower()\n",
    "\n",
    "def normalize_repeated_chars(text):\n",
    "    \"\"\"\n",
    "    Normalize repeated characters (more than 2) to exactly 2.\n",
    "    Example: \"enakkkk\" -> \"enakk\"\n",
    "    \"\"\"\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1\\1', (text or ''))\n",
    "\n",
    "def tokenizingText(text):\n",
    "    \"\"\"Tokenize text using IndoBERT tokenizer.\"\"\"\n",
    "    text = (text or '').strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "def filteringText(tokens):\n",
    "    \"\"\"\n",
    "    Remove stopwords from the token list.\n",
    "    Uses Indonesian and English stopwords with additional custom stopwords.\n",
    "    \"\"\"\n",
    "    listStopwords = set(stopwords.words('indonesian'))\n",
    "    listStopwords.update(set(stopwords.words('english')))\n",
    "    additional_stopwords = ['iya','yaa','gk','gak','g','dr','nya','na','sih','ku','di','ga','ya',\n",
    "                          'gaa','loh','kah','woi','woii','woy','pas','c','deh','eh']\n",
    "    listStopwords.update(additional_stopwords)\n",
    "    return [word for word in tokens if word not in listStopwords]\n",
    "\n",
    "def toSentence(list_words):\n",
    "    \"\"\"Convert a list of words back to a sentence.\"\"\"\n",
    "    return ' '.join(word for word in list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f334e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load slang words dictionary\n",
    "with open('data_clean/slangwords.json', 'r', encoding='utf-8') as f:\n",
    "    slangwords = json.load(f)\n",
    "\n",
    "def fix_slangwords(text):\n",
    "    \"\"\"\n",
    "    Replace slang words with their standard equivalents.\n",
    "    Uses a predefined dictionary of slang words and their standard forms.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "        \n",
    "    words = text.split()\n",
    "    fixed_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if word.lower() in slangwords:\n",
    "            fixed_words.append(slangwords[word.lower()])\n",
    "        else:\n",
    "            fixed_words.append(word)\n",
    "\n",
    "    return ' '.join(fixed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "555bb9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Text Cleaning and Normalization\n",
    "\n",
    "# Clean the text by removing special characters, URLs, etc.\n",
    "df['text_clean'] = df['review'].apply(cleaningText)\n",
    "\n",
    "# Convert all text to lowercase\n",
    "df['text_casefoldingText'] = df['text_clean'].apply(casefoldingText)\n",
    "\n",
    "# Normalize repeated characters (e.g., 'sooo' -> 'soo')\n",
    "df['text_normrepeat'] = df['text_casefoldingText'].apply(normalize_repeated_chars)\n",
    "\n",
    "# Replace slang words with their standard forms\n",
    "df['text_slangwords'] = df['text_normrepeat'].apply(fix_slangwords)\n",
    "\n",
    "# 4.2 Translation of English Words (HF en->id)\n",
    "# Translate any English words to Indonesian while preserving the rest of the text\n",
    "def apply_translation(text):\n",
    "    parts = split_preserve_delimiters(text)\n",
    "    items = merge_english_spans(parts)\n",
    "    # HF translate_spans no longer uses dest, it is fixed to en->id\n",
    "    return translate_spans(items)\n",
    "\n",
    "df['text_translated'] = df['text_slangwords'].apply(apply_translation)\n",
    "\n",
    "# 4.3 Tokenization and Stopword Removal\n",
    "# Tokenize the text\n",
    "df['text_tokenizingText'] = df['text_translated'].apply(tokenizingText)\n",
    "\n",
    "# Remove stopwords\n",
    "df['text_stopword'] = df['text_tokenizingText'].apply(filteringText)\n",
    "\n",
    "# 4.4 Final Text Processing\n",
    "# Convert tokens back to sentences\n",
    "df['text_akhir'] = df['text_stopword'].apply(toSentence)\n",
    "\n",
    "# Store the final cleaned text\n",
    "df['text_clean'] = df['text_translated']\n",
    "df['sentence'] = df['text_akhir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "114b523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 De-duplicate and remove original review column\n",
    "# Drop duplicate rows based on the cleaned text to avoid redundancy\n",
    "if 'text_clean' in df.columns:\n",
    "    df.drop_duplicates(subset=['text_clean'], inplace=True, ignore_index=True)\n",
    "\n",
    "# Remove the original 'review' column to avoid redundancy after cleaning\n",
    "if 'review' in df.columns:\n",
    "    df.drop(columns=['review'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0ae459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6 Remove single-word reviews\n",
    "# Filter out rows where the cleaned text has only one word\n",
    "if 'text_clean' in df.columns:\n",
    "    word_counts = df['text_clean'].fillna('').str.findall(r'\\b\\w+\\b').str.len()\n",
    "    df = df[word_counts > 1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "535ebf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_slangwords(text):\n",
    "    words = text.split()\n",
    "    fixed_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if word.lower() in slangwords:\n",
    "            fixed_words.append(slangwords[word.lower()])\n",
    "        else:\n",
    "            fixed_words.append(word)\n",
    "\n",
    "    fixed_text = ' '.join(fixed_words)\n",
    "    return fixed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32490fb",
   "metadata": {},
   "source": [
    "## 4. Text Processing Pipeline\n",
    "\n",
    "This section applies all the preprocessing steps to the review data in a sequential manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f9dd0a",
   "metadata": {},
   "source": [
    "## 5. English Translation (Optional)\n",
    "\n",
    "This section translates the cleaned Indonesian text to English for further analysis. This is optional and can be skipped if not needed.\n",
    "\n",
    "```python\n",
    "# Note: This section requires the Helsinki-NLP/opus-mt-id-en model\n",
    "# Uncomment and run this cell if you need English translations\n",
    "\"\"\"\n",
    "from transformers import pipeline as hf_pipeline\n",
    "\n",
    "# Initialize the translation pipeline\n",
    "translator = hf_pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-id-en\")\n",
    "\n",
    "def translate_texts(texts, batch_size=32, max_length=256):\n",
    "    \"\"\"\n",
    "    Translate a batch of texts from Indonesian to English.\n",
    "    Handles batching to avoid memory issues with large datasets.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    n = len(texts)\n",
    "    for i in range(0, n, batch_size):\n",
    "        batch = [t if isinstance(t, str) else \"\" for t in texts[i:i+batch_size]]\n",
    "        outs = translator(batch, max_length=max_length)\n",
    "        results.extend([o.get(\"translation_text\", \"\") for o in outs])\n",
    "    return results\n",
    "\n",
    "# Translate the cleaned text to English\n",
    "print(\"Translating to English (this may take a while)...\")\n",
    "texts_to_translate = df['text_clean'].fillna(\"\").tolist()\n",
    "df['text_clean_en'] = translate_texts(texts_to_translate, batch_size=32)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3cd0048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of processed data:\n",
      "\n",
      "Processed data saved to: data_clean\\all_reviews_cleaned.csv\n",
      "Total unique reviews processed (after de-dup): 9183\n"
     ]
    }
   ],
   "source": [
    "## 6. Save Processed Data\n",
    "\n",
    "# Display a sample of the processed data\n",
    "print(\"Sample of processed data:\")\n",
    "cols_to_show = [c for c in ['text_clean', 'sentence'] if c in df.columns]\n",
    "df[cols_to_show].sample(5)\n",
    "\n",
    "# Save the cleaned data to a CSV file\n",
    "output_dir = \"data_clean\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "output_csv = os.path.join(output_dir, \"all_reviews_cleaned.csv\")\n",
    "output_cols = ['text_clean']\n",
    "\n",
    "# Include English translation if available\n",
    "if 'text_clean_en' in df.columns:\n",
    "    output_cols.append('text_clean_en')\n",
    "\n",
    "# Guard in case some columns are missing\n",
    "output_cols = [c for c in output_cols if c in df.columns]\n",
    "\n",
    "df_out = df[output_cols].copy()\n",
    "df_out.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "print(f\"\\nProcessed data saved to: {output_csv}\")\n",
    "print(f\"Total unique reviews processed (after de-dup): {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "826001c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Vocab] Wrote 10100 tokens to data_clean\\vocab.txt\n"
     ]
    }
   ],
   "source": [
    "# 7. Generate Vocabulary / Keywords and save to TXT (tokens only, no counts)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vocab_input_csv = os.path.join(\"data_clean\", \"all_reviews_cleaned.csv\")\n",
    "vocab_output_txt = os.path.join(\"data_clean\", \"vocab.txt\")\n",
    "\n",
    "# Load cleaned data\n",
    "_vdf = pd.read_csv(vocab_input_csv, encoding=\"utf-8-sig\")\n",
    "if \"text_clean\" not in _vdf.columns:\n",
    "    raise ValueError(\"'text_clean' column not found in data_clean/all_reviews_cleaned.csv\")\n",
    "\n",
    "_texts = _vdf[\"text_clean\"].astype(str).fillna(\"\").tolist()\n",
    "\n",
    "# Build vocabulary (tokens only)\n",
    "_vec = CountVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    ngram_range=(1, 1),  # set to (1, 2) if you want bigrams too\n",
    "    min_df=1,\n",
    "    max_df=1.0,\n",
    "    lowercase=True,\n",
    ")\n",
    "_vec.fit(_texts)\n",
    "_vocab = _vec.get_feature_names_out()\n",
    "\n",
    "# Sort tokens alphabetically for determinism\n",
    "_tokens = sorted(_vocab.tolist())\n",
    "\n",
    "# Write tokens only (one token per line)\n",
    "os.makedirs(os.path.dirname(vocab_output_txt), exist_ok=True)\n",
    "with open(vocab_output_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "    for tok in _tokens:\n",
    "        f.write(f\"{tok}\\n\")\n",
    "\n",
    "print(f\"[Vocab] Wrote {len(_tokens)} tokens to {vocab_output_txt}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
